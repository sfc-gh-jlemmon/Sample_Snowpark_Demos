{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5Mvn0qyNlF7"
      },
      "source": [
        "# Install Snowpark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "tVqlAMGIEHTT"
      },
      "outputs": [],
      "source": [
        "!pip install snowflake-snowpark-python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1G1RE5iPNpfs"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1ijF-sgJNf-U"
      },
      "source": [
        "# Connect to Snowflake via SnowPark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPtS8X-WEsz6",
        "outputId": "17bc376e-73f5-4db7-8813-6f883df8b573"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import json\n",
        "\n",
        "\n",
        "# --->  PYSPARK\n",
        "\n",
        "# import pyspark.sql.functions as f\n",
        "# from pyspark.sql import SparkSession\n",
        "# from pyspark.sql.functions import udf,col\n",
        "# from pyspark.sql.types import IntegerType\n",
        "# spark = SparkSession.builder.appName(\"DataEngeering1\").getOrCreate()\n",
        "\n",
        "# <---  PYSPARK\n",
        "\n",
        "import snowflake.snowpark.functions as f\n",
        "from snowflake.snowpark import Session, DataFrame\n",
        "from snowflake.snowpark.functions import udf, col\n",
        "from snowflake.snowpark.types import IntegerType\n",
        "from snowflake.snowpark.functions import call_udf\n",
        "\n",
        "\n",
        "# <----- Make these changes before running the notebook -------\n",
        "# 1. Change Connection params to match your environment\n",
        "\n",
        "# <----------------------------------------------------------------------------\n",
        "\n",
        "Warehouse_Name = 'MY_ETL_WH'\n",
        "DB_NAME = 'DEMO_SNOWPARK'\n",
        "\n",
        "CONNECTION_PARAMETERS1 = {\n",
        "    \"host\": \"<YourAccount>.snowflakecomputing.com\",\n",
        "    'account': '<YourAccount>',\n",
        "    'user': '<Your_UserID>',\n",
        "    'password': '<Your_Password>',\n",
        "    'role': 'SYSADMIN',\n",
        "}\n",
        "\n",
        "with open('creds.json') as credsfile:\n",
        "    data = json.load(credsfile)\n",
        "    username = data['username']\n",
        "    password = data['password']\n",
        "    account = data[\"account\"]\n",
        "\n",
        "    CONNECTION_PARAMETERS = {\n",
        "        'account': account,\n",
        "        'user': username,\n",
        "        'password': password,\n",
        "        'role': 'SYSADMIN',\n",
        "    }\n",
        "\n",
        "print(\"Connecting to Snowflake.....\\n\")\n",
        "session = Session.builder.configs(CONNECTION_PARAMETERS).create()\n",
        "print(\"Connected Successfully!...\\n\\n\")\n",
        "\n",
        "\n",
        "\n",
        "sql_cmd = \"CREATE OR REPLACE WAREHOUSE {} WAREHOUSE_SIZE = 'X-Small' \".format(Warehouse_Name)\n",
        "session.sql(sql_cmd).collect() \n",
        "\n",
        "sql_cmd = \"CREATE OR REPLACE DATABASE {}\".format(DB_NAME)\n",
        "session.sql(sql_cmd).collect() \n",
        "\n",
        "sql_cmd = \"USE SCHEMA {}.PUBLIC\".format(DB_NAME)\n",
        "session.sql(sql_cmd).collect() \n",
        "\n",
        "sql_cmd = \"USE WAREHOUSE {}\".format(Warehouse_Name)\n",
        "session.sql(sql_cmd).collect() \n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Start Data Engineering Process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# 1 - INCREASE COMPUTE TO 4 NODES\n",
        "print(\"Resizing to from XS(1 Node) to MEDIUM(4 Nodes) ..\\n\")\n",
        "\n",
        "sql_cmd = \"ALTER WAREHOUSE {} SET WAREHOUSE_SIZE = 'LARGE' WAIT_FOR_COMPLETION = TRUE\".format(Warehouse_Name)\n",
        "session.sql(sql_cmd).collect()  \n",
        "\n",
        "print(\"Completed!...\\n\\n\")\n",
        "\n",
        "\n",
        "# 2 - READ & JOIN 2 LARGE TABLES (600M & 1M rows)\n",
        "print(\"Joining, Aggregating with 2 large tables(600M & 1M rows) & Writing results to new table(80M rows) ..\\n\")\n",
        "\n",
        "dfLineItems = session.table(\"SFC_SAMPLES_SAMPLE_DATA.TPCH_SF100.LINEITEM\")  # 600 Million Rows\n",
        "dfSuppliers = session.table(\"SFC_SAMPLES_SAMPLE_DATA.TPCH_SF100.SUPPLIER\")  # 1 Million Rows\n",
        "\n",
        "print('Lineitems Table: %s rows' % \"{:,}\".format(dfLineItems.count()))\n",
        "print('Suppliers Table: %s rows' % \"{:,}\".format(dfSuppliers.count()))\n",
        "\n",
        "# 3 - JOIN TABLES\n",
        "dfJoinTables = dfLineItems.join(dfSuppliers,\n",
        "                                dfLineItems.col(\"L_SUPPKEY\") == dfSuppliers.col(\"S_SUPPKEY\"))  \n",
        "\n",
        "# # 4 - SUMMARIZE THE DATA BY SUPPLIER, PART, SUM, MIN & MAX\n",
        "dfSummary = dfJoinTables.groupBy(\"S_NAME\", \"L_PARTKEY\").agg([\n",
        "     f.sum(\"L_QUANTITY\").alias(\"TOTAL_QTY\"),\n",
        "     f.min(\"L_QUANTITY\").alias(\"MIN_QTY\"),\n",
        "     f.max(\"L_QUANTITY\").alias(\"MAX_QTY\")\n",
        "])\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **â†‘ Compute is NOT used** up to this point. (Lazy Execution Model) !!!"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Storing the Results in Table or Showing results triggers the compute & previous steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "start_time = time.time()\n",
        "# 5 - WRITE THE RESULTS TO A NEW TABLE ( 80 Million Rows)\n",
        "# <-- This is when all the previous operations are compiled & executed as a single job\n",
        "dfSummary.write.mode(\"overwrite\").saveAsTable(\"SALES_SUMMARY\")\n",
        "print(\"Completed!...\\n\\n\")\n",
        "\n",
        "# 6 - QUERY THE RESULTS (80 Million Rows)\n",
        "print(\"Query the results..\\n\")\n",
        "dfSales = session.table(\"SALES_SUMMARY\")\n",
        "dfSales.show()\n",
        "end_time = time.time()\n",
        "\n",
        "print(\"Completed!...\\n\\n\")\n",
        "\n",
        "# 7 - SCALE DOWN COMPUTE TO 1 NODE\n",
        "print(\"Reducing the warehouse to XS..\\n\")\n",
        "sql_cmd = \"ALTER WAREHOUSE {} SET WAREHOUSE_SIZE = 'XSMALL'\".format(Warehouse_Name)\n",
        "session.sql(sql_cmd).collect()  \n",
        "\n",
        "print(\"Completed!...\\n\")\n",
        "\n",
        "print(\"--- %s seconds to Join, Summarize & Write Results to a new Table --- \\n\" % int(end_time - start_time))\n",
        "print(\"--- %s Rows Written to SALES_SUMMARY table\" % \"{:,}\".format(dfSales.count()))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# That's all there is to it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Clean it all up\n",
        "sql_cmd = \"DROP WAREHOUSE {} \".format(Warehouse_Name)\n",
        "session.sql(sql_cmd).collect() \n",
        "\n",
        "sql_cmd = \"DROP DATABASE {}\".format(DB_NAME)\n",
        "session.sql(sql_cmd).collect() "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
