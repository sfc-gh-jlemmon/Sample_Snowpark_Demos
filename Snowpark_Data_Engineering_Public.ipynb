{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5Mvn0qyNlF7"
      },
      "source": [
        "# Install Snowpark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "tVqlAMGIEHTT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting snowflake-snowpark-python\n",
            "  Downloading snowflake_snowpark_python-1.4.0-py3-none-any.whl (284 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.3/284.3 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools>=40.6.0 in ./.conda/lib/python3.8/site-packages (from snowflake-snowpark-python) (66.0.0)\n",
            "Collecting cloudpickle<=2.0.0,>=1.6.0\n",
            "  Using cached cloudpickle-2.0.0-py3-none-any.whl (25 kB)\n",
            "Collecting snowflake-connector-python<4.0.0,>=2.7.12\n",
            "  Using cached snowflake_connector_python-3.0.3-cp38-cp38-macosx_10_14_x86_64.whl (15.4 MB)\n",
            "Requirement already satisfied: wheel in ./.conda/lib/python3.8/site-packages (from snowflake-snowpark-python) (0.38.4)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.1.0 in ./.conda/lib/python3.8/site-packages (from snowflake-snowpark-python) (4.5.0)\n",
            "Collecting idna<4,>=2.5\n",
            "  Using cached idna-3.4-py3-none-any.whl (61 kB)\n",
            "Collecting oscrypto<2.0.0\n",
            "  Using cached oscrypto-1.3.0-py2.py3-none-any.whl (194 kB)\n",
            "Collecting asn1crypto<2.0.0,>0.24.0\n",
            "  Using cached asn1crypto-1.5.1-py2.py3-none-any.whl (105 kB)\n",
            "Collecting pyjwt<3.0.0\n",
            "  Using cached PyJWT-2.6.0-py3-none-any.whl (20 kB)\n",
            "Collecting urllib3<1.27,>=1.21.1\n",
            "  Using cached urllib3-1.26.15-py2.py3-none-any.whl (140 kB)\n",
            "Collecting filelock<4,>=3.5\n",
            "  Using cached filelock-3.12.0-py3-none-any.whl (10 kB)\n",
            "Collecting pytz\n",
            "  Using cached pytz-2023.3-py2.py3-none-any.whl (502 kB)\n",
            "Collecting requests<3.0.0\n",
            "  Using cached requests-2.29.0-py3-none-any.whl (62 kB)\n",
            "Collecting cffi<2.0.0,>=1.9\n",
            "  Using cached cffi-1.15.1-cp38-cp38-macosx_10_9_x86_64.whl (178 kB)\n",
            "Collecting pycryptodomex!=3.5.0,<4.0.0,>=3.2\n",
            "  Using cached pycryptodomex-3.17-cp35-abi3-macosx_10_9_x86_64.whl (1.6 MB)\n",
            "Requirement already satisfied: packaging in ./.conda/lib/python3.8/site-packages (from snowflake-connector-python<4.0.0,>=2.7.12->snowflake-snowpark-python) (23.1)\n",
            "Collecting pyOpenSSL<24.0.0,>=16.2.0\n",
            "  Using cached pyOpenSSL-23.1.1-py3-none-any.whl (57 kB)\n",
            "Collecting certifi>=2017.4.17\n",
            "  Using cached certifi-2022.12.7-py3-none-any.whl (155 kB)\n",
            "Collecting charset-normalizer<3,>=2\n",
            "  Using cached charset_normalizer-2.1.1-py3-none-any.whl (39 kB)\n",
            "Collecting cryptography<41.0.0,>=3.1.0\n",
            "  Using cached cryptography-40.0.2-cp36-abi3-macosx_10_12_x86_64.whl (2.8 MB)\n",
            "Collecting pycparser\n",
            "  Using cached pycparser-2.21-py2.py3-none-any.whl (118 kB)\n",
            "Installing collected packages: pytz, asn1crypto, urllib3, pyjwt, pycryptodomex, pycparser, oscrypto, idna, filelock, cloudpickle, charset-normalizer, certifi, requests, cffi, cryptography, pyOpenSSL, snowflake-connector-python, snowflake-snowpark-python\n",
            "Successfully installed asn1crypto-1.5.1 certifi-2022.12.7 cffi-1.15.1 charset-normalizer-2.1.1 cloudpickle-2.0.0 cryptography-40.0.2 filelock-3.12.0 idna-3.4 oscrypto-1.3.0 pyOpenSSL-23.1.1 pycparser-2.21 pycryptodomex-3.17 pyjwt-2.6.0 pytz-2023.3 requests-2.29.0 snowflake-connector-python-3.0.3 snowflake-snowpark-python-1.4.0 urllib3-1.26.15\n"
          ]
        }
      ],
      "source": [
        "!pip install snowflake-snowpark-python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1G1RE5iPNpfs"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1ijF-sgJNf-U"
      },
      "source": [
        "# Connect to Snowflake via SnowPark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPtS8X-WEsz6",
        "outputId": "17bc376e-73f5-4db7-8813-6f883df8b573"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Connecting to Snowflake.....\n",
            "\n",
            "Connected Successfully!...\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[Row(status='Statement executed successfully.')]"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import time\n",
        "import json\n",
        "\n",
        "\n",
        "# --->  PYSPARK\n",
        "\n",
        "# import pyspark.sql.functions as f\n",
        "# from pyspark.sql import SparkSession\n",
        "# from pyspark.sql.functions import udf,col\n",
        "# from pyspark.sql.types import IntegerType\n",
        "# spark = SparkSession.builder.appName(\"DataEngeering1\").getOrCreate()\n",
        "\n",
        "# <---  PYSPARK\n",
        "\n",
        "import snowflake.snowpark.functions as f\n",
        "from snowflake.snowpark import Session, DataFrame\n",
        "from snowflake.snowpark.functions import udf, col\n",
        "from snowflake.snowpark.types import IntegerType\n",
        "from snowflake.snowpark.functions import call_udf\n",
        "\n",
        "\n",
        "# <----- Make these changes before running the notebook -------\n",
        "# 1. Change Connection params to match your environment\n",
        "\n",
        "# <----------------------------------------------------------------------------\n",
        "\n",
        "Warehouse_Name = 'MY_ETL_WH'\n",
        "DB_NAME = 'DEMO_SNOWPARK'\n",
        "\n",
        "CONNECTION_PARAMETERS1 = {\n",
        "    \"host\": \"<YourAccount>.snowflakecomputing.com\",\n",
        "    'account': '<YourAccount>',\n",
        "    'user': '<Your_UserID>',\n",
        "    'password': '<Your_Password>',\n",
        "    'role': 'SYSADMIN',\n",
        "}\n",
        "\n",
        "with open('creds.json') as f:\n",
        "    data = json.load(f)\n",
        "    username = data['username']\n",
        "    password = data['password']\n",
        "    account = data[\"account\"]\n",
        "\n",
        "    CONNECTION_PARAMETERS = {\n",
        "        'account': account,\n",
        "        'user': username,\n",
        "        'password': password,\n",
        "        'role': 'SYSADMIN',\n",
        "    }\n",
        "\n",
        "print(\"Connecting to Snowflake.....\\n\")\n",
        "session = Session.builder.configs(CONNECTION_PARAMETERS).create()\n",
        "print(\"Connected Successfully!...\\n\\n\")\n",
        "\n",
        "\n",
        "\n",
        "sql_cmd = \"CREATE OR REPLACE WAREHOUSE {} WAREHOUSE_SIZE = 'X-Small' \".format(Warehouse_Name)\n",
        "session.sql(sql_cmd).collect() \n",
        "\n",
        "sql_cmd = \"CREATE OR REPLACE DATABASE {}\".format(DB_NAME)\n",
        "session.sql(sql_cmd).collect() \n",
        "\n",
        "sql_cmd = \"USE SCHEMA {}.PUBLIC\".format(DB_NAME)\n",
        "session.sql(sql_cmd).collect() \n",
        "\n",
        "sql_cmd = \"USE WAREHOUSE {}\".format(Warehouse_Name)\n",
        "session.sql(sql_cmd).collect() \n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Start Data Engineering Process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Resizing to from XS(1 Node) to MEDIUM(4 Nodes) ..\n",
            "\n",
            "Completed!...\n",
            "\n",
            "\n",
            "Joining, Aggregating with 2 large tables(600M & 1M rows) & Writing results to new table(80M rows) ..\n",
            "\n",
            "Lineitems Table: 600,037,902 rows\n",
            "Suppliers Table: 1,000,000 rows\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# 1 - INCREASE COMPUTE TO 4 NODES\n",
        "print(\"Resizing to from XS(1 Node) to MEDIUM(4 Nodes) ..\\n\")\n",
        "\n",
        "sql_cmd = \"ALTER WAREHOUSE {} SET WAREHOUSE_SIZE = 'LARGE' WAIT_FOR_COMPLETION = TRUE\".format(Warehouse_Name)\n",
        "session.sql(sql_cmd).collect()  \n",
        "\n",
        "print(\"Completed!...\\n\\n\")\n",
        "\n",
        "\n",
        "# 2 - READ & JOIN 2 LARGE TABLES (600M & 1M rows)\n",
        "print(\"Joining, Aggregating with 2 large tables(600M & 1M rows) & Writing results to new table(80M rows) ..\\n\")\n",
        "\n",
        "dfLineItems = session.table(\"SFC_SAMPLES_SAMPLE_DATA.TPCH_SF100.LINEITEM\")  # 600 Million Rows\n",
        "dfSuppliers = session.table(\"SFC_SAMPLES_SAMPLE_DATA.TPCH_SF100.SUPPLIER\")  # 1 Million Rows\n",
        "\n",
        "print('Lineitems Table: %s rows' % \"{:,}\".format(dfLineItems.count()))\n",
        "print('Suppliers Table: %s rows' % \"{:,}\".format(dfSuppliers.count()))\n",
        "\n",
        "# 3 - JOIN TABLES\n",
        "dfJoinTables = dfLineItems.join(dfSuppliers,\n",
        "                                dfLineItems.col(\"L_SUPPKEY\") == dfSuppliers.col(\"S_SUPPKEY\"))  \n",
        "\n",
        "# 4 - SUMMARIZE THE DATA BY SUPPLIER, PART, SUM, MIN & MAX\n",
        "dfSummary = dfJoinTables.groupBy(\"S_NAME\", \"L_PARTKEY\").agg([\n",
        "    f.sum(\"L_QUANTITY\").alias(\"TOTAL_QTY\"),\n",
        "    f.min(\"L_QUANTITY\").alias(\"MIN_QTY\"),\n",
        "    f.max(\"L_QUANTITY\").alias(\"MAX_QTY\"),\n",
        "])\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **↑ Compute is NOT used** up to this point. (Lazy Execution Model) !!!"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Storing the Results in Table or Showing results triggers the compute & previous steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed!...\n",
            "\n",
            "\n",
            "Query the results..\n",
            "\n",
            "--------------------------------------------------------------------------\n",
            "|\"S_NAME\"            |\"L_PARTKEY\"  |\"TOTAL_QTY\"  |\"MIN_QTY\"  |\"MAX_QTY\"  |\n",
            "--------------------------------------------------------------------------\n",
            "|Supplier#000564493  |5564492      |207.00       |1.00       |50.00      |\n",
            "|Supplier#000676924  |2676923      |193.00       |4.00       |47.00      |\n",
            "|Supplier#000639927  |9139908      |282.00       |10.00      |50.00      |\n",
            "|Supplier#000542648  |1292646      |243.00       |6.00       |49.00      |\n",
            "|Supplier#000697355  |10697354     |151.00       |3.00       |50.00      |\n",
            "|Supplier#000062149  |19562110     |203.00       |6.00       |47.00      |\n",
            "|Supplier#000548120  |15798074     |277.00       |7.00       |49.00      |\n",
            "|Supplier#000685994  |2935987      |100.00       |19.00      |50.00      |\n",
            "|Supplier#000383108  |2133105      |151.00       |16.00      |39.00      |\n",
            "|Supplier#000971264  |8971263      |235.00       |1.00       |40.00      |\n",
            "--------------------------------------------------------------------------\n",
            "\n",
            "Completed!...\n",
            "\n",
            "\n",
            "Reducing the warehouse to XS..\n",
            "\n",
            "Completed!...\n",
            "\n",
            "--- 15 seconds to Join, Summarize & Write Results to a new Table --- \n",
            "\n",
            "--- 79,975,543 Rows Written to SALES_SUMMARY table\n"
          ]
        }
      ],
      "source": [
        "start_time = time.time()\n",
        "# 5 - WRITE THE RESULTS TO A NEW TABLE ( 80 Million Rows)\n",
        "# <-- This is when all the previous operations are compiled & executed as a single job\n",
        "dfSummary.write.mode(\"overwrite\").saveAsTable(\"SALES_SUMMARY\")\n",
        "print(\"Completed!...\\n\\n\")\n",
        "\n",
        "# 6 - QUERY THE RESULTS (80 Million Rows)\n",
        "print(\"Query the results..\\n\")\n",
        "dfSales = session.table(\"SALES_SUMMARY\")\n",
        "dfSales.show()\n",
        "end_time = time.time()\n",
        "\n",
        "print(\"Completed!...\\n\\n\")\n",
        "\n",
        "# 7 - SCALE DOWN COMPUTE TO 1 NODE\n",
        "print(\"Reducing the warehouse to XS..\\n\")\n",
        "sql_cmd = \"ALTER WAREHOUSE {} SET WAREHOUSE_SIZE = 'XSMALL'\".format(Warehouse_Name)\n",
        "session.sql(sql_cmd).collect()  \n",
        "\n",
        "print(\"Completed!...\\n\")\n",
        "\n",
        "print(\"--- %s seconds to Join, Summarize & Write Results to a new Table --- \\n\" % int(end_time - start_time))\n",
        "print(\"--- %s Rows Written to SALES_SUMMARY table\" % \"{:,}\".format(dfSales.count()))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# That's all there is to it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Row(status='DEMO_SNOWPARK successfully dropped.')]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Clean it all up\n",
        "sql_cmd = \"DROP WAREHOUSE {} \".format(Warehouse_Name)\n",
        "session.sql(sql_cmd).collect() \n",
        "\n",
        "sql_cmd = \"DROP DATABASE {}\".format(DB_NAME)\n",
        "session.sql(sql_cmd).collect() "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
